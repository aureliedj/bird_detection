{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training object detection model\r\n",
        "\r\n",
        "Reference GitHub: https://medium.com/fullstackai/how-to-train-an-object-detector-with-your-own-coco-dataset-in-pytorch-319e7090da5"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, faster_rcnn\r\n",
        "from torch  import nn\r\n",
        "\r\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights= FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\r\n",
        "\r\n",
        "## change first convolution to take 3*3 channels as inputs --> 9\r\n",
        "model.backbone.body.conv1 = nn.Conv2d(9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\r\n",
        "# get number of input features for the classifier\r\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n",
        "# replace the pre-trained head with a new one\r\n",
        "model.roi_heads.box_predictor = faster_rcnn.FastRCNNPredictor(in_features, 2)\r\n",
        "\r\n",
        "## check trainabe parameters\r\n",
        "params = [p for p in model.parameters() if p.requires_grad]"
      ],
      "outputs": [],
      "execution_count": 98,
      "metadata": {
        "gather": {
          "logged": 1673539297067
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import torch.utils.data\r\n",
        "import torchvision\r\n",
        "from PIL import Image\r\n",
        "from pycocotools.coco import COCO"
      ],
      "outputs": [],
      "execution_count": 100,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673539418974
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myOwnDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, root, annotation, transforms=None):\r\n",
        "        self.root = root\r\n",
        "        self.transforms = transforms\r\n",
        "        self.coco = COCO(annotation)\r\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        # Own coco file\r\n",
        "        coco = self.coco\r\n",
        "        # Image ID\r\n",
        "        img_id = self.ids[index]\r\n",
        "        # List: get annotation id from coco\r\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\r\n",
        "        # Dictionary: target coco_annotation file for an image\r\n",
        "        coco_annotation = coco.loadAnns(ann_ids)\r\n",
        "        # path for input image\r\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\r\n",
        "        # open the input image\r\n",
        "        img = Image.open(os.path.join(self.root, path))\r\n",
        "\r\n",
        "        # number of objects in the image\r\n",
        "        num_objs = len(coco_annotation)\r\n",
        "\r\n",
        "        # Bounding boxes for objects\r\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\r\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\r\n",
        "        boxes = []\r\n",
        "        for i in range(num_objs):\r\n",
        "            xmin = coco_annotation[i]['bbox'][0]\r\n",
        "            ymin = coco_annotation[i]['bbox'][1]\r\n",
        "            xmax = xmin + coco_annotation[i]['bbox'][2]\r\n",
        "            ymax = ymin + coco_annotation[i]['bbox'][3]\r\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\r\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\r\n",
        "        # Labels (In my case, I only one class: target class or background)\r\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\r\n",
        "        # Tensorise img_id\r\n",
        "        img_id = torch.tensor([img_id])\r\n",
        "        # Size of bbox (Rectangular)\r\n",
        "        areas = []\r\n",
        "        for i in range(num_objs):\r\n",
        "            areas.append(coco_annotation[i]['area'])\r\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\r\n",
        "        # Iscrowd\r\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n",
        "\r\n",
        "        # Annotation is in dictionary format\r\n",
        "        my_annotation = {}\r\n",
        "        my_annotation[\"boxes\"] = boxes\r\n",
        "        my_annotation[\"labels\"] = labels\r\n",
        "        my_annotation[\"image_id\"] = img_id\r\n",
        "        my_annotation[\"area\"] = areas\r\n",
        "        my_annotation[\"iscrowd\"] = iscrowd\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            img = self.transforms(img)\r\n",
        "\r\n",
        "        return img, my_annotation\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.ids)"
      ],
      "outputs": [],
      "execution_count": 115,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673539901434
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd() + '/../data/birds/annotations/all'\r\n",
        "pathin = root + '/../coco_annotations.json'"
      ],
      "outputs": [],
      "execution_count": 116,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673539902512
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform():\r\n",
        "    custom_transforms = []\r\n",
        "    custom_transforms.append(torchvision.transforms.ToTensor())\r\n",
        "    return torchvision.transforms.Compose(custom_transforms)\r\n",
        "\r\n",
        "ds = myOwnDataset(root, pathin, get_transform())\r\n",
        "\r\n",
        "# collate_fn needs for batch\r\n",
        "def collate_fn(batch):\r\n",
        "    return tuple(zip(*batch))\r\n",
        "\r\n",
        "data_loader = torch.utils.data.DataLoader(ds,\r\n",
        "            batch_size = 2,\r\n",
        "            shuffle = True,\r\n",
        "            num_workers = 0,\r\n",
        "            collate_fn = collate_fn)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "loading annotations into memory...\nDone (t=0.15s)\ncreating index...\nindex created!\n"
        }
      ],
      "execution_count": 138,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673540253622
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 classes; Only target class or background\r\n",
        "num_classes = 2\r\n",
        "num_epochs = 10\r\n",
        "model = get_model_object_detection(num_classes)\r\n",
        "\r\n",
        "# select device (whether GPU or CPU)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\r\n",
        "# move model to the right device\r\n",
        "model.to(device)\r\n",
        "    \r\n",
        "# parameters\r\n",
        "params = [p for p in model.parameters() if p.requires_grad]\r\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\r\n",
        "\r\n",
        "len_dataloader = len(data_loader)\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    model.train()\r\n",
        "    i = 0    \r\n",
        "    for imgs, annotations in data_loader:\r\n",
        "        i += 1\r\n",
        "        imgs = list(img.to(device) for img in imgs)\r\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\r\n",
        "        loss_dict = model(imgs, annotations)\r\n",
        "        losses = sum(loss for loss in loss_dict.values())\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        losses.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 139,
          "data": {
            "text/plain": "((tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.2627, 0.2157, 0.2157],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1608, 0.1882, 0.1843],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.1922, 0.2000]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.2941, 0.2471, 0.2471],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1922, 0.2196, 0.2157],\n           [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.2078, 0.2157]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.3333, 0.2314, 0.2314],\n           [0.0000, 0.0000, 0.0000,  ..., 0.2314, 0.2039, 0.2000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1922, 0.2039, 0.2118]]]),\n  tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.1922, 0.1804, 0.1529],\n           [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.1647, 0.1412],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1843, 0.1490, 0.1216]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.1412, 0.1137],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1647, 0.1255, 0.1020],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.1137, 0.0863]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.1843, 0.1922, 0.1647],\n           [0.0000, 0.0000, 0.0000,  ..., 0.1922, 0.1765, 0.1529],\n           [0.0000, 0.0000, 0.0000,  ..., 0.2078, 0.1608, 0.1333]]])),\n ({'boxes': tensor([[356.4202, 163.0350, 374.3191, 178.9883]]),\n   'labels': tensor([1]),\n   'image_id': tensor([622]),\n   'area': tensor([285.]),\n   'iscrowd': tensor([0])},\n  {'boxes': tensor([[162.9496,  25.3381, 173.7410,  37.5683]]),\n   'labels': tensor([1]),\n   'image_id': tensor([1094]),\n   'area': tensor([131.]),\n   'iscrowd': tensor([0])}))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 139,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673540258314
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
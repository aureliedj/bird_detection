{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\r\n",
        "from torch  import nn\r\n",
        "\r\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights= FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\r\n",
        "\r\n",
        "## change first convolution to take 3*3 channels as inputs --> 9\r\n",
        "model.backbone.body.conv1 = nn.Conv2d(9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673532670456
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "import os.path\r\n",
        "from typing import Any, Callable, List, Optional, Tuple\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "from torchvision.datasets.vision import VisionDataset\r\n",
        "\r\n",
        "from pycocotools.coco import COCO"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673533152124
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetection(VisionDataset):\r\n",
        "    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\r\n",
        "\r\n",
        "    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (string): Root directory where images are downloaded to.\r\n",
        "        annFile (string): Path to json annotation file.\r\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\r\n",
        "            and returns a transformed version. E.g, ``transforms.PILToTensor``\r\n",
        "        target_transform (callable, optional): A function/transform that takes in the\r\n",
        "            target and transforms it.\r\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\r\n",
        "            and returns a transformed version.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        root: str,\r\n",
        "        annFile: str,\r\n",
        "        transform: Optional[Callable] = None,\r\n",
        "        target_transform: Optional[Callable] = None,\r\n",
        "        transforms: Optional[Callable] = None,\r\n",
        "    ) -> None:\r\n",
        "        super().__init__(root, transforms, transform, target_transform)\r\n",
        "        from pycocotools.coco import COCO\r\n",
        "\r\n",
        "        self.coco = COCO(annFile)\r\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\r\n",
        "\r\n",
        "    def _load_image(self, id: int) -> Image.Image:\r\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\r\n",
        "        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\r\n",
        "\r\n",
        "    def _load_target(self, id: int) -> List[Any]:\r\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\r\n",
        "        id = self.ids[index]\r\n",
        "        image = self._load_image(id)\r\n",
        "        target = self._load_target(id)\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            image, target = self.transforms(image, target)\r\n",
        "\r\n",
        "        return image, target\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.ids)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1673533286085
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd() + '/../data/birds/annotations'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}